{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from torch import FloatTensor, LongTensor\n",
    "\n",
    "\n",
    "class Data(BaseModel):\n",
    "    laser_params: FloatTensor\n",
    "    emiss: FloatTensor\n",
    "    uids: LongTensor\n",
    "    wavelength: FloatTensor\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.forward_model import ForwardModel\n",
    "from models.backward_model import BackwardModel\n",
    "from models.base_model import BaseModel\n",
    "from config import Config\n",
    "from models.model_factory import ModelFactory\n",
    "from pathlib import Path\n",
    "from einops.layers.torch import Rearrange\n",
    "from typing import Optional\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import utils\n",
    "from config import Config\n",
    "from dto.data import Data\n",
    "from utils import FileUtils, split\n",
    "from utils.utils import Stage\n",
    "\n",
    "# CKPT_PATH = \"../data/models/model.ckpt\" # \"../weights/forward/epoch=0122-step=1230-val_loss=0.28432.ckpt\"\n",
    "# MODEL_PATH = \"data/mymodels\"\n",
    "#load data\n",
    "config = Config()\n",
    "\n",
    "data_file = Path(f\"../{config.data_folder}/{config.data_file}\")\n",
    "data = torch.load(data_file)\n",
    "data_struct = Data(\n",
    "        laser_params=data[\"normalized_laser_params\"],\n",
    "        emiss=data[\"interpolated_emissivity\"],\n",
    "        uids=data[\"uids\"],\n",
    "        wavelength=data[\"wavelength\"],\n",
    "    )\n",
    "    # Set the number of wavelengths\n",
    "    # config.num_wavelens = data[\"interpolated_emissivity\"].shape[-1]\n",
    "#     return Data(\n",
    "#         laser_params=data[\"normalized_laser_params\"],\n",
    "#         emiss=data[\"interpolated_emissivity\"],\n",
    "#         uids=data[\"uids\"],\n",
    "#         wavelength=data[\"wavelength\"],\n",
    "#     )\n",
    "\n",
    "# data = read_pt_data(config.data_folder, config.data_file)\n",
    "splits = split(len(data_struct.laser_params))\n",
    "train, val, test = [\n",
    "    TensorDataset(\n",
    "        data_struct.laser_params[splits[s].start : splits[s].stop],\n",
    "        data_struct.emiss[splits[s].start : splits[s].stop],\n",
    "        data_struct.uids[splits[s].start : splits[s].stop],\n",
    "    )\n",
    "    for s in (\"train\", \"val\", \"test\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "class BasicConvBlock(nn.Module):\n",
    "    \n",
    "    ''' The BasicConvBlock takes an input with in_channels, applies some blocks of convolutional layers \n",
    "    to reduce it to out_channels and sum it up to the original input. \n",
    "    If their sizes mismatch, then the input goes into an identity. \n",
    "    \n",
    "    Basically The BasicConvBlock will implement the regular basic Conv Block + \n",
    "    the shortcut block that does the dimension matching job (option A or B) when dimension changes between 2 blocks\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1, option='A'):\n",
    "        super(BasicConvBlock, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)),\n",
    "            ('bn1', nn.BatchNorm2d(out_channels)),\n",
    "            ('act1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "            ('bn2', nn.BatchNorm2d(out_channels))\n",
    "        ]))\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        '''  When input and output spatial dimensions don't match, we have 2 options, with stride:\n",
    "            - A) Use identity shortcuts with zero padding to increase channel dimension.    \n",
    "            - B) Use 1x1 convolution to increase channel dimension (projection shortcut).\n",
    "         '''\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            if option == 'A':\n",
    "                # Use identity shortcuts with zero padding to increase channel dimension.\n",
    "                pad_to_add = out_channels//4\n",
    "                ''' ::2 is doing the job of stride = 2\n",
    "                F.pad apply padding to (W,H,C,N).\n",
    "                \n",
    "                The padding lengths are specified in reverse order of the dimensions,\n",
    "                F.pad(x[:, :, ::2, ::2], (0,0, 0,0, pad,pad, 0,0))\n",
    "\n",
    "                [width_beginning, width_end, height_beginning, height_end, channel_beginning, channel_end, batchLength_beginning, batchLength_end ]\n",
    "\n",
    "                '''\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                            F.pad(x[:, :, ::2, ::2], (0,0, 0,0, pad_to_add, pad_to_add, 0,0)))\n",
    "            if option == 'B':\n",
    "                self.shortcut = nn.Sequential(OrderedDict([\n",
    "                    ('s_conv1', nn.Conv2d(in_channels, 2*out_channels, kernel_size=1, stride=stride, padding=0, bias=False)),\n",
    "                    ('s_bn1', nn.BatchNorm2d(2*out_channels))\n",
    "                ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        # sum it up with shortcut layer\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "        ResNet-56 architecture for CIFAR-10 Dataset of shape 32*32*3\n",
    "    \"\"\"\n",
    "    def __init__(self, block_type, num_blocks, in_channels=14, out_channels=512, kernel_size=1, stride=1, padding=1):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(in_channels, 16, kernel_size=kernel_size, stride=1, padding=1, bias=False)\n",
    "        self.bn0 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.block1 = self.__build_layer(block_type, 16, num_blocks[0], starting_stride=1)\n",
    "        \n",
    "        self.block2 = self.__build_layer(block_type, 32, num_blocks[1], starting_stride=2)\n",
    "        \n",
    "        self.block3 = self.__build_layer(block_type, 64, num_blocks[2], starting_stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.linear = nn.Linear(64, 10)\n",
    "    \n",
    "    def __build_layer(self, block_type, out_channels, num_blocks, starting_stride):\n",
    "        \n",
    "        strides_list_for_current_block = [starting_stride] + [1]*(num_blocks-1)\n",
    "        ''' Above line will generate an array whose first element is starting_stride\n",
    "        And it will have (num_blocks-1) more elements each of value 1\n",
    "         '''\n",
    "        # print('strides_list_for_current_block ', strides_list_for_current_block)\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        for stride in strides_list_for_current_block:\n",
    "            layers.append(block_type(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn0(self.conv0(x)))\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)        \n",
    "        out = self.block3(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": "Expected 2 dimensions, got 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/spencersong/metamaterials_ai/src/walkthrough.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B128.3.29.45/home/spencersong/metamaterials_ai/src/walkthrough.ipynb#ch0000006vscode-remote?line=6'>7</a>\u001b[0m \u001b[39m# device = 'cpu'\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B128.3.29.45/home/spencersong/metamaterials_ai/src/walkthrough.ipynb#ch0000006vscode-remote?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B128.3.29.45/home/spencersong/metamaterials_ai/src/walkthrough.ipynb#ch0000006vscode-remote?line=8'>9</a>\u001b[0m summary(model, (\u001b[39m3\u001b[39;49m, \u001b[39m32\u001b[39;49m, \u001b[39m32\u001b[39;49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/meta/lib/python3.10/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torchsummary/torchsummary.py?line=67'>68</a>\u001b[0m model\u001b[39m.\u001b[39mapply(register_hook)\n\u001b[1;32m     <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torchsummary/torchsummary.py?line=69'>70</a>\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torchsummary/torchsummary.py?line=70'>71</a>\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torchsummary/torchsummary.py?line=71'>72</a>\u001b[0m model(\u001b[39m*\u001b[39;49mx)\n\u001b[1;32m     <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torchsummary/torchsummary.py?line=73'>74</a>\u001b[0m \u001b[39m# remove these hooks\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torchsummary/torchsummary.py?line=74'>75</a>\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/module.py:1128\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1124'>1125</a>\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/anaconda3/envs/meta/lib/python3.10/site-packages/einops/layers/torch.py:14\u001b[0m, in \u001b[0;36mRearrange.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/einops/layers/torch.py?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/einops/layers/torch.py?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_for_scriptable_torch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recipe, \u001b[39minput\u001b[39;49m, reduction_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrearrange\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/meta/lib/python3.10/site-packages/einops/_torch_specific.py:77\u001b[0m, in \u001b[0;36mapply_for_scriptable_torch\u001b[0;34m(recipe, tensor, reduction_type)\u001b[0m\n\u001b[1;32m     <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/einops/_torch_specific.py?line=73'>74</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_for_scriptable_torch\u001b[39m(recipe: TransformRecipe, tensor: torch\u001b[39m.\u001b[39mTensor, reduction_type: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/einops/_torch_specific.py?line=74'>75</a>\u001b[0m     backend \u001b[39m=\u001b[39m TorchJitBackend\n\u001b[1;32m     <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/einops/_torch_specific.py?line=75'>76</a>\u001b[0m     init_shapes, reduced_axes, axes_reordering, added_axes, final_shapes \u001b[39m=\u001b[39m \\\n\u001b[0;32m---> <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/einops/_torch_specific.py?line=76'>77</a>\u001b[0m         _reconstruct_from_shape_uncached(recipe, backend\u001b[39m.\u001b[39;49mshape(tensor))\n\u001b[1;32m     <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/einops/_torch_specific.py?line=77'>78</a>\u001b[0m     tensor \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mreshape(tensor, init_shapes)\n\u001b[1;32m     <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/einops/_torch_specific.py?line=78'>79</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(reduced_axes) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/meta/lib/python3.10/site-packages/einops/einops.py:163\u001b[0m, in \u001b[0;36m_reconstruct_from_shape_uncached\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/einops/einops.py?line=160'>161</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/einops/einops.py?line=161'>162</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shape) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_composite_axes):\n\u001b[0;32m--> <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/einops/einops.py?line=162'>163</a>\u001b[0m         \u001b[39mraise\u001b[39;00m EinopsError(\u001b[39m'\u001b[39m\u001b[39mExpected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_composite_axes), \u001b[39mlen\u001b[39m(shape)))\n\u001b[1;32m    <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/einops/einops.py?line=164'>165</a>\u001b[0m ellipsis_shape: List[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='file:///home/spencersong/anaconda3/envs/meta/lib/python3.10/site-packages/einops/einops.py?line=165'>166</a>\u001b[0m \u001b[39mfor\u001b[39;00m input_axis, (known_axes, unknown_axes) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_composite_axes):\n",
      "\u001b[0;31mEinopsError\u001b[0m: Expected 2 dimensions, got 4"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "        Rearrange(\"b c -> b c 1 1\"),\n",
    "        ResNet(block_type=BasicConvBlock, num_blocks=[9,9,9], in_channels=14, out_channels=512),\n",
    "        nn.Sigmoid()\n",
    "        )\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = 'cpu'\n",
    "model.to(device)\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3561369123.py, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [29]\u001b[0;36m\u001b[0m\n\u001b[0;31m    with torch.no_grad():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "train_samples_num = 45000\n",
    "val_samples_num = 5000\n",
    "train_costs, val_costs = [], []\n",
    "train_loader = DataLoader(train, batch_size=1000, shuffle=True)\n",
    "val_loader = DataLoader(val, batch_size=1000, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=1000, shuffle=True)\n",
    "    \n",
    "#Training phase.    \n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    train_running_loss = 0\n",
    "    correct_train = 0    \n",
    "    model.train().cuda()\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        \"\"\" for every mini-batch during the training phase, we typically want to explicitly set the gradients \n",
    "        to zero before starting to do backpropragation \"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Start the forward pass\n",
    "        prediction = model(inputs)\n",
    "                    \n",
    "        loss = criterion(prediction, labels)\n",
    "        \n",
    "        # do backpropagation and update weights with step()\n",
    "        loss.backward()         \n",
    "        optimizer.step()\n",
    "        \n",
    "        # print('outputs on which to apply torch.max ', prediction)\n",
    "        # find the maximum along the rows, use dim=1 to torch.max()\n",
    "        _, predicted_outputs = torch.max(prediction.data, 1)\n",
    "        \n",
    "        # Update the running corrects \n",
    "        correct_train += (predicted_outputs == labels).float().sum().item()\n",
    "        \n",
    "        ''' Compute batch loss\n",
    "        multiply each average batch loss with batch-length. \n",
    "        The batch-length is inputs.size(0) which gives the number total images in each batch. \n",
    "        Essentially I am un-averaging the previously calculated Loss '''\n",
    "        train_running_loss += (loss.data.item() * inputs.shape[0])\n",
    "\n",
    "\n",
    "    train_epoch_loss = train_running_loss / train_samples_num\n",
    "    \n",
    "    train_costs.append(train_epoch_loss)\n",
    "    \n",
    "    train_acc =  correct_train / train_samples_num\n",
    "\n",
    "    # Now check trained weights on the validation set\n",
    "    val_running_loss = 0\n",
    "    correct_val = 0\n",
    "    \n",
    "    model.eval().cuda()\n",
    "a\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass.\n",
    "            prediction = model(inputs)\n",
    "\n",
    "            # Compute the loss.\n",
    "            loss = criterion(prediction, labels)\n",
    "\n",
    "            # Compute validation accuracy.\n",
    "            _, predicted_outputs = torch.max(prediction.data, 1)\n",
    "            correct_val += (predicted_outputs == labels).float().sum().item()\n",
    "\n",
    "        # Compute batch loss.\n",
    "        val_running_loss += (loss.data.item() * inputs.shape[0])\n",
    "\n",
    "        val_epoch_loss = val_running_loss / val_samples_num\n",
    "        val_costs.append(val_epoch_loss)\n",
    "        val_acc =  correct_val / val_samples_num\n",
    "    \n",
    "    info = \"[Epoch {}/{}]: train-loss = {:0.6f} | train-acc = {:0.3f} | val-loss = {:0.6f} | val-acc = {:0.3f}\"\n",
    "    \n",
    "    print(info.format(epoch+1, EPOCHS, train_epoch_loss, train_acc, val_epoch_loss, val_acc))\n",
    "    \n",
    "    torch.save(model.state_dict(), '/content/checkpoint_gpu_{}'.format(epoch + 1)) \n",
    "                                                            \n",
    "torch.save(model.state_dict(), '/content/resnet-56_weights_gpu')  \n",
    "    \n",
    "return train_costs, val_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67e7f89d5252d36c0a7a22a0bddaf9ce6b4bb105a5aa258c373ad72ae6e9c8c8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('meta')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
